{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "collapsed_sections": [
        "Zfy8t0ClDyXm",
        "Ug2tA5DKECWW",
        "VY79NukHEF3T",
        "QVmDGpKC1teG"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cgrundman/ml-stethoscope/blob/main/code/003_Machine_Learned_Stethoscope.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine-Learned Stethoscope\n",
        "\n",
        "Authored by "
      ],
      "metadata": {
        "id": "sIBHxGsOtXmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from: https://www.kaggle.com/code/eatmygoose/cnn-detection-of-wheezes-and-crackles"
      ],
      "metadata": {
        "id": "EsWURzKEfLCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Statements"
      ],
      "metadata": {
        "id": "WT1k9ETSbq05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "metadata": {
        "id": "XLNZYShyfouj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wave\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "q_Kl-641e2k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "cUfri4iRbyTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/content/drive/MyDrive/Machine Learned Stethoscope/Dataset/Respiratory_Sound_Database')"
      ],
      "metadata": {
        "id": "a0H4wEfOflJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_diagnosis = pd.read_csv('/content/drive/MyDrive/Machine Learned Stethoscope/Dataset/Respiratory_Sound_Database/demographic_info.txt', names = \n",
        "                 ['Patient number', 'Age', 'Sex' , 'Adult BMI (kg/m2)', 'Child Weight (kg)' , 'Child Height (cm)'],\n",
        "                 delimiter = ' ')\n",
        "\n",
        "diagnosis = pd.read_csv('/content/drive/MyDrive/Machine Learned Stethoscope/Dataset/Respiratory_Sound_Database/patient_diagnosis.csv', names = ['Patient number', 'Diagnosis'])"
      ],
      "metadata": {
        "id": "V_S4LkA4ghHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df =  df_no_diagnosis.join(diagnosis.set_index('Patient number'), on = 'Patient number', how = 'left')\n",
        "df['Diagnosis'].value_counts()"
      ],
      "metadata": {
        "id": "E3-ZmnIKi0Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = '/content/drive/MyDrive/Machine Learned Stethoscope/Dataset/Respiratory_Sound_Database/audio_and_txt_files'\n",
        "filenames = [s.split('.')[0] for s in os.listdir(path = root) if '.txt' in s]"
      ],
      "metadata": {
        "id": "ypgJWhLEi6XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TBD"
      ],
      "metadata": {
        "id": "5F3XBEjI86Aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sort the Data\n",
        "\n",
        "In this section, we create lists of the data to create a better model. This section create a series of lists from the data and labels available in this dataset. We create a list of all of the data present in all files, compile this imformation into a list for overall labels, and create a data base based on wheeze and crackle information."
      ],
      "metadata": {
        "id": "e-ax0ktui90P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the data for every file, sound and txt files\n",
        "def Extract_Annotation_Data(file_name, root):\n",
        "    tokens = file_name.split('_')\n",
        "    recording_info = pd.DataFrame(data = [tokens], \n",
        "                                  columns = ['Patient number', \n",
        "                                             'Recording index', \n",
        "                                             'Chest location',\n",
        "                                             'Acquisition mode',\n",
        "                                             'Recording equipment'])\n",
        "    # .txt file information\n",
        "    recording_annotations = pd.read_csv(os.path.join(root, file_name + '.txt'), \n",
        "                                        names = ['Start', \n",
        "                                                 'End', \n",
        "                                                 'Crackles', \n",
        "                                                 'Wheezes'], \n",
        "                                        delimiter= '\\t')\n",
        "    return (recording_info, recording_annotations)"
      ],
      "metadata": {
        "id": "VEAca6lujY3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists and dictionaries\n",
        "info_list = []\n",
        "rec_annotations = [] \n",
        "rec_annotations_dict = {} # rec_dictionary\n",
        "\n",
        "# Populate lists and dictionaries\n",
        "for filename in filenames:\n",
        "    (rec_info,annotations) = Extract_Annotation_Data(filename, root)\n",
        "    info_list.append(rec_info)\n",
        "    rec_annotations.append(annotations)\n",
        "    rec_annotations_dict[filename] = annotations\n",
        "\n",
        "# Concatinate \n",
        "recording_info = pd.concat(info_list, axis = 0)\n",
        "recording_info.head()"
      ],
      "metadata": {
        "id": "d67dyfHtjdGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists\n",
        "no_label_list = []\n",
        "crack_list = []\n",
        "wheeze_list = []\n",
        "both_sym_list = []\n",
        "filename_list = []\n",
        "\n",
        "# Label all the data based on wheeze and crackle annotations\n",
        "for f in filenames:\n",
        "    d = rec_annotations_dict[f]\n",
        "    no_labels = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 0)].index)\n",
        "    n_crackles = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 0)].index)\n",
        "    n_wheezes = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 1)].index)\n",
        "    both_sym = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 1)].index)\n",
        "    no_label_list.append(no_labels)\n",
        "    crack_list.append(n_crackles)\n",
        "    wheeze_list.append(n_wheezes)\n",
        "    both_sym_list.append(both_sym)\n",
        "    filename_list.append(f)"
      ],
      "metadata": {
        "id": "oqcxTaTqj1a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe based on wheeze and crackle labels\n",
        "file_label_df = pd.DataFrame(data = {'filename':filename_list, \n",
        "                                     'no label':no_label_list, \n",
        "                                     'crackles only':crack_list, \n",
        "                                     'wheezes only':wheeze_list, \n",
        "                                     'crackles and wheezees':both_sym_list})"
      ],
      "metadata": {
        "id": "hwYF9N_uj5jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_labels = file_label_df[(file_label_df['crackles only'] != 0) | (file_label_df['wheezes only'] != 0) | (file_label_df['crackles and wheezees'] != 0)]\n",
        "\n",
        "# Display totals of the file labels\n",
        "file_label_df.sum()"
      ],
      "metadata": {
        "id": "UYP9AeJ1kFPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read .wav Files\n",
        "\n",
        "In this section, we define the utility functions for reading .wav files (especially 24bit .wav). The audio data has been taken from multiple devices haing differing sampling rates. We resample all the data to the same rate in order to train our model.\n",
        "\n",
        "24bit sample rates are so chalenging, because it is not a bit-wise multple (or pwer of 2). This extrapolates this data to match the other sources. Making tha sample rate to a base 2 creates an even relationship in steps and second."
      ],
      "metadata": {
        "id": "AgS3Naztkd6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wave\n",
        "import math\n",
        "import scipy.io.wavfile as wf\n",
        "\n",
        "# Read the wave files and return a 32bit array float array\n",
        "def read_wav_file(str_filename, target_rate):\n",
        "    wav = wave.open(str_filename, mode = 'r')\n",
        "    (sample_rate, data) = extract2FloatArr(wav, str_filename)\n",
        "    \n",
        "    if (sample_rate != target_rate):\n",
        "        ( _ , data) = resample(sample_rate, data, target_rate)\n",
        "        \n",
        "    wav.close()\n",
        "    return (target_rate, data.astype(np.float32))\n",
        "\n",
        "# Resample all files to the target sample rate\n",
        "def resample(current_rate, data, target_rate):\n",
        "    x_original = np.linspace(0,100,len(data))\n",
        "    x_resampled = np.linspace(0,100, int(len(data) * (target_rate / current_rate)))\n",
        "    resampled = np.interp(x_resampled, x_original, data)\n",
        "    return (target_rate, resampled.astype(np.float32))\n",
        "\n",
        "# Extract .wav data into float array\n",
        "def extract2FloatArr(lp_wave, str_filename):\n",
        "    (bps, channels) = bitrate_channels(lp_wave)\n",
        "    \n",
        "    if bps in [1,2,4]:\n",
        "        (rate, data) = wf.read(str_filename)\n",
        "        divisor_dict = {1:255, 2:32768}\n",
        "        if bps in [1,2]:\n",
        "            divisor = divisor_dict[bps]\n",
        "            data = np.divide(data, float(divisor)) # clamp to [0.0,1.0]        \n",
        "        return (rate, data)\n",
        "    \n",
        "    elif bps == 3: \n",
        "        # 24bpp wave\n",
        "        return read24bitwave(lp_wave)\n",
        "    \n",
        "    else:\n",
        "        raise Exception('Unrecognized wave format: {} bytes per sample'.format(bps))\n",
        "        \n",
        "# Truncate the 24bit samples to 16bit precision, only needed for 24bit files\n",
        "def read24bitwave(lp_wave):\n",
        "    nFrames = lp_wave.getnframes()\n",
        "    buf = lp_wave.readframes(nFrames)\n",
        "    reshaped = np.frombuffer(buf, np.int8).reshape(nFrames,-1)\n",
        "    short_output = np.empty((nFrames, 2), dtype = np.int8)\n",
        "    short_output[:,:] = reshaped[:, -2:]\n",
        "    short_output = short_output.view(np.int16)\n",
        "\n",
        "    # Return numpy array to save memory via array slicing\n",
        "    return (lp_wave.getframerate(), np.divide(short_output, 32768).reshape(-1))  \n",
        "    \n",
        "# Find bitrate for certain .wav file\n",
        "def bitrate_channels(lp_wave):\n",
        "    bps = (lp_wave.getsampwidth() / lp_wave.getnchannels()) # bytes per sample\n",
        "    return (bps, lp_wave.getnchannels())\n",
        "\n",
        "# Split data into section defined by start and end\n",
        "def slice_data(start, end, raw_data,  sample_rate):\n",
        "    max_ind = len(raw_data) \n",
        "    start_ind = min(int(start * sample_rate), max_ind)\n",
        "    end_ind = min(int(end * sample_rate), max_ind)\n",
        "    return raw_data[start_ind: end_ind]"
      ],
      "metadata": {
        "id": "rwmi2e90kW2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribution of respiratory cycle lengths"
      ],
      "metadata": {
        "id": "r_uZWzX3kvJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize list\n",
        "duration_list = []\n",
        "\n",
        "# Populate list of recording durations\n",
        "for i in range(len(rec_annotations)):\n",
        "    current = rec_annotations[i]\n",
        "    duration = current['End'] - current['Start']\n",
        "    duration_list.extend(duration)\n",
        "\n",
        "# Display of the delay times max, min, and cutoff\n",
        "duration_list = np.array(duration_list) # convert list to np array\n",
        "plt.hist(duration_list, bins = 50)\n",
        "print('longest cycle:{}'.format(max(duration_list)))\n",
        "print('shortest cycle:{}'.format(min(duration_list)))\n",
        "threshold = 5\n",
        "print('Fraction of samples less than {} seconds:{}'.format(threshold, np.sum(duration_list < threshold)/len(duration_list)))"
      ],
      "metadata": {
        "id": "JZQqR57vkx1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "XRLHv1gvkNhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Melspectrogram implementation (With VTLP)\n",
        "\n",
        "This section creates Melspectrograms created based on the work of Jaitly & Hinton (2013). To read more about this:\n",
        "https://www.cs.utoronto.ca/~hinton/absps/perturb.pdf"
      ],
      "metadata": {
        "id": "AlW9j6pxlE4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.signal\n",
        "\n",
        "# Trun a sample into a melsprectrum\n",
        "def sample2MelSpectrum(cycle_info, sample_rate, n_filters, vtlp_params):\n",
        "    n_rows = 175 # 7500 cutoff\n",
        "    n_window = 512 #~25 ms window\n",
        "    (f, t, Sxx) = scipy.signal.spectrogram(cycle_info[0],fs = sample_rate, nfft= n_window, nperseg=n_window)\n",
        "    Sxx = Sxx[:n_rows,:].astype(np.float32) # sift out coefficients above 7500hz, Sxx has 196 columns\n",
        "    mel_log = FFT2MelSpectrogram(f[:n_rows], Sxx, sample_rate, n_filters, vtlp_params)[1]\n",
        "    mel_min = np.min(mel_log)\n",
        "    mel_max = np.max(mel_log)\n",
        "    diff = mel_max - mel_min\n",
        "    norm_mel_log = (mel_log - mel_min) / diff if (diff > 0) else np.zeros(shape = (n_filters,Sxx.shape[1]))\n",
        "    if (diff == 0):\n",
        "        print('Error: sample data is completely empty')\n",
        "    labels = [cycle_info[1], cycle_info[2]] # crackles, wheezes flags\n",
        "    return (np.reshape(norm_mel_log, (n_filters,Sxx.shape[1],1)).astype(np.float32), # 196x64x1 matrix\n",
        "            label2onehot(labels)) \n",
        "\n",
        "# Calculate frequency into melspectrum space\n",
        "def Freq2Mel(freq):\n",
        "    return 1125 * np.log(1 + freq / 700)\n",
        "\n",
        "# Calculate melspectrum space back into frequency\n",
        "def Mel2Freq(mel):\n",
        "    exponents = mel / 1125\n",
        "    return 700 * (np.exp(exponents) - 1)\n",
        "\n",
        "# Takes an array of original mel  frequencies and returns warped version of them\n",
        "def VTLP_shift(mel_freq, alpha, f_high, sample_rate):\n",
        "    nyquist_f = sample_rate / 2\n",
        "    warp_factor = min(alpha, 1)\n",
        "    threshold_freq = f_high * warp_factor / alpha\n",
        "    lower = mel_freq * alpha\n",
        "    higher = nyquist_f - (nyquist_f - mel_freq) * ((nyquist_f - f_high * warp_factor) / (nyquist_f - f_high * (warp_factor / alpha)))\n",
        "    \n",
        "    warped_mel = np.where(mel_freq <= threshold_freq, lower, higher)\n",
        "    return warped_mel.astype(np.float32)\n",
        "\n",
        "# mel_space_freq: the mel frequencies (HZ) of the filter banks, in addition to \n",
        "#                 the two maximum and minimum frequency values\n",
        "# fft_bin_frequencies: the bin freqencies of the FFT output\n",
        "# Generates a 2d numpy array, with each row containing each filter bank\n",
        "def GenerateMelFilterBanks(mel_space_freq, fft_bin_frequencies):\n",
        "    n_filters = len(mel_space_freq) - 2\n",
        "    coeff = []\n",
        "    #Triangular filter windows\n",
        "    #ripped from http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\n",
        "    for mel_index in range(n_filters):\n",
        "        m = int(mel_index + 1)\n",
        "        filter_bank = []\n",
        "        for f in fft_bin_frequencies:\n",
        "            if(f < mel_space_freq[m-1]):\n",
        "                hm = 0\n",
        "            elif(f < mel_space_freq[m]):\n",
        "                hm = (f - mel_space_freq[m-1]) / (mel_space_freq[m] - mel_space_freq[m-1])\n",
        "            elif(f < mel_space_freq[m + 1]):\n",
        "                hm = (mel_space_freq[m+1] - f) / (mel_space_freq[m + 1] - mel_space_freq[m])\n",
        "            else:\n",
        "                hm = 0\n",
        "            filter_bank.append(hm)\n",
        "        coeff.append(filter_bank)\n",
        "    return np.array(coeff, dtype = np.float32)\n",
        "        \n",
        "#Transform spectrogram into mel spectrogram -> (frequencies, spectrum)\n",
        "#vtlp_params = (alpha, f_high), vtlp will not be applied if set to None\n",
        "def FFT2MelSpectrogram(f, Sxx, sample_rate, n_filterbanks, vtlp_params = None):\n",
        "    (max_mel, min_mel)  = (Freq2Mel(max(f)), Freq2Mel(min(f)))\n",
        "    mel_bins = np.linspace(min_mel, max_mel, num = (n_filterbanks + 2))\n",
        "    #Convert mel_bins to corresponding frequencies in hz\n",
        "    mel_freq = Mel2Freq(mel_bins)\n",
        "    \n",
        "    if(vtlp_params is None):\n",
        "        filter_banks = GenerateMelFilterBanks(mel_freq, f)\n",
        "    else:\n",
        "        #Apply VTLP\n",
        "        (alpha, f_high) = vtlp_params\n",
        "        warped_mel = VTLP_shift(mel_freq, alpha, f_high, sample_rate)\n",
        "        filter_banks = GenerateMelFilterBanks(warped_mel, f)\n",
        "        \n",
        "    mel_spectrum = np.matmul(filter_banks, Sxx)\n",
        "    return (mel_freq[1:-1], np.log10(mel_spectrum  + float(10e-12)))\n",
        "    \n",
        "# Labels proved too difficult to train (model keep convergining to statistical mean)\n",
        "# Flattened to onehot labels since the number of combinations is very low\n",
        "def label2onehot(c_w_flags):\n",
        "    c = c_w_flags[0]\n",
        "    w = c_w_flags[1]\n",
        "    if((c == False) & (w == False)):\n",
        "        return [1,0,0,0]\n",
        "    elif((c == True) & (w == False)):\n",
        "        return [0,1,0,0]\n",
        "    elif((c == False) & (w == True)):\n",
        "        return [0,0,1,0]\n",
        "    else:\n",
        "        return [0,0,0,1]"
      ],
      "metadata": {
        "id": "6HzGwzDLlGih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation utility functions"
      ],
      "metadata": {
        "id": "b0MFbIaKlaoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to split each individual sound file into separate sound clips containing one respiratory cycle each\n",
        "# output: [filename, (sample_data:np.array, start:float, end:float, crackles:bool(float), wheezes:bool(float)) (...) ]\n",
        "def get_sound_samples(recording_annotations, file_name, root, sample_rate):\n",
        "    sample_data = [file_name]\n",
        "    (rate, data) = read_wav_file(os.path.join(root, file_name + '.wav'), sample_rate)\n",
        "    \n",
        "    for i in range(len(recording_annotations.index)):\n",
        "        row = recording_annotations.loc[i]\n",
        "        start = row['Start']\n",
        "        end = row['End']\n",
        "        crackles = row['Crackles']\n",
        "        wheezes = row['Wheezes']\n",
        "        audio_chunk = slice_data(start, end, data, rate)\n",
        "        sample_data.append((audio_chunk, start,end,crackles,wheezes))\n",
        "    return sample_data\n",
        "\n",
        "#Fits each respiratory cycle into a fixed length audio clip, splits may be performed and zero padding is added if necessary\n",
        "#original:(arr,c,w) -> output:[(arr,c,w),(arr,c,w)]\n",
        "def split_and_pad(original, desiredLength, sampleRate):\n",
        "    output_buffer_length = int(desiredLength * sampleRate)\n",
        "    soundclip = original[0]\n",
        "    n_samples = len(soundclip)\n",
        "    total_length = n_samples / sampleRate #length of cycle in seconds\n",
        "    n_slices = int(math.ceil(total_length / desiredLength)) #get the minimum number of slices needed\n",
        "    samples_per_slice = n_samples // n_slices\n",
        "    src_start = 0 #Staring index of the samples to copy from the original buffer\n",
        "    output = [] #Holds the resultant slices\n",
        "    for i in range(n_slices):\n",
        "        src_end = min(src_start + samples_per_slice, n_samples)\n",
        "        length = src_end - src_start\n",
        "        copy = generate_padded_samples(soundclip[src_start:src_end], output_buffer_length)\n",
        "        output.append((copy, original[1], original[2]))\n",
        "        src_start += length\n",
        "    return output\n",
        "\n",
        "def generate_padded_samples(source, output_length):\n",
        "    copy = np.zeros(output_length, dtype = np.float32)\n",
        "    src_length = len(source)\n",
        "    frac = src_length / output_length\n",
        "    if(frac < 0.5):\n",
        "        #tile forward sounds to fill empty space\n",
        "        cursor = 0\n",
        "        while(cursor + src_length) < output_length:\n",
        "            copy[cursor:(cursor + src_length)] = source[:]\n",
        "            cursor += src_length\n",
        "    else:\n",
        "        copy[:src_length] = source[:]\n",
        "    #\n",
        "    return copy"
      ],
      "metadata": {
        "id": "2kPA4uPkle-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data augmentation\n",
        "Two basic forms employed : audio stretching (speeding up or down) as well as Vocal Tract Length perturbation"
      ],
      "metadata": {
        "id": "ln_8cYD4lp2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creates a copy of each time slice, but stretches or contracts it by a random amount\n",
        "def gen_time_stretch(original, sample_rate, max_percent_change):\n",
        "    stretch_amount = 1 + np.random.uniform(-1,1) * (max_percent_change / 100)\n",
        "    (_, stretched) = resample(sample_rate, original, int(sample_rate * stretch_amount)) \n",
        "    return stretched\n",
        "\n",
        "#Same as above, but applies it to a list of samples\n",
        "def augment_list(audio_with_labels, sample_rate, percent_change, n_repeats):\n",
        "    augmented_samples = []\n",
        "    for i in range(n_repeats):\n",
        "        addition = [(gen_time_stretch(t[0], sample_rate, percent_change), t[1], t[2] ) for t in audio_with_labels]\n",
        "        augmented_samples.extend(addition)\n",
        "    return augmented_samples\n",
        "\n",
        "#Takes a list of respiratory cycles, and splits and pads each cycle into fixed length buffers (determined by desiredLength(seconds))\n",
        "#Then takes the split and padded sample and transforms it into a mel spectrogram\n",
        "#VTLP_alpha_range = [Lower, Upper] (Bounds of random selection range), \n",
        "#VTLP_high_freq_range = [Lower, Upper] (-)\n",
        "#output:[(arr:float[],c:float_bool,w:float_bool),(arr,c,w)]\n",
        "def split_and_pad_and_apply_mel_spect(original, desiredLength, sampleRate, VTLP_alpha_range = None, VTLP_high_freq_range = None, n_repeats = 1):\n",
        "    output = []\n",
        "    for i in range(n_repeats):\n",
        "        for d in original:\n",
        "            lst_result = split_and_pad(d, desiredLength, sampleRate) #Time domain\n",
        "            if( (VTLP_alpha_range is None) | (VTLP_high_freq_range is None) ):\n",
        "                #Do not apply VTLP\n",
        "                VTLP_params = None\n",
        "            else:\n",
        "                #Randomly generate VLTP parameters\n",
        "                alpha = np.random.uniform(VTLP_alpha_range[0], VTLP_alpha_range[1])\n",
        "                high_freq = np.random.uniform(VTLP_high_freq_range[0], VTLP_high_freq_range[1])\n",
        "                VTLP_params = (alpha, high_freq)\n",
        "            freq_result = [sample2MelSpectrum(d, sampleRate, 50, VTLP_params) for d in lst_result] #Freq domain\n",
        "            output.extend(freq_result)\n",
        "    return output"
      ],
      "metadata": {
        "id": "F-yZ19oFlxDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str_file = filenames[11]\n",
        "lp_test = get_sound_samples(rec_annotations_dict[str_file], str_file, root, 22000)\n",
        "lp_cycles = [(d[0], d[3], d[4]) for d in lp_test[1:]]\n",
        "soundclip = lp_cycles[1][0]\n",
        "\n",
        "n_window = 512\n",
        "sample_rate = 22000\n",
        "(f, t, Sxx) = scipy.signal.spectrogram(soundclip, fs = 22000, nfft= n_window, nperseg=n_window)\n",
        "print(sum(f < 7000))\n",
        "\n",
        "plt.figure(figsize = (20,10))\n",
        "plt.subplot(1,2,1)\n",
        "mel_banks = FFT2MelSpectrogram(f[:175], Sxx[:175,:], sample_rate, 50)[1]\n",
        "plt.imshow(mel_banks, aspect = 1)\n",
        "plt.title('No VTLP')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "mel_banks = FFT2MelSpectrogram(f[:175], Sxx[:175,:], sample_rate, 50, vtlp_params = (0.9,3500))[1]\n",
        "plt.imshow(mel_banks, aspect = 1)\n",
        "plt.title('With VTLP')"
      ],
      "metadata": {
        "id": "Kj1NXJiel9nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility used to import all training samples"
      ],
      "metadata": {
        "id": "WcALJdy5mFgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def extract_all_training_samples(filenames, annotation_dict, root, target_rate, desired_length, train_test_ratio = 0.2):\n",
        "    cycle_list = []\n",
        "    for file in filenames:\n",
        "        data = get_sound_samples(annotation_dict[file], file, root, target_rate)\n",
        "        cycles_with_labels = [(d[0], d[3], d[4]) for d in data[1:]]\n",
        "        cycle_list.extend(cycles_with_labels)\n",
        "    \n",
        "    #Sort into respective classes\n",
        "    no_labels = [c for c in cycle_list if ((c[1] == 0) & (c[2] == 0))]\n",
        "    c_only = [c for c in cycle_list if ((c[1] == 1) & (c[2] == 0))] \n",
        "    w_only = [c for c in cycle_list if ((c[1] == 0) & (c[2] == 1))]\n",
        "    c_w = [c for c in cycle_list if ((c[1] == 1) & (c[2] == 1))]\n",
        "    \n",
        "    #Count of labels across all cycles, actual recording time also follows similar ratios\n",
        "    #none:3642\n",
        "    #crackles:1864 \n",
        "    #wheezes:886\n",
        "    #both:506\n",
        "    none_train, none_test = train_test_split(no_labels, test_size = train_test_ratio)\n",
        "    c_train, c_test  = train_test_split(c_only, test_size = train_test_ratio)\n",
        "    w_train, w_test  = train_test_split(w_only, test_size = train_test_ratio)\n",
        "    c_w_train, c_w_test  = train_test_split(c_w, test_size = train_test_ratio)\n",
        "    \n",
        "    #Training section (Data augmentation procedures)\n",
        "    #Augment w_only and c_w groups to match the size of c_only\n",
        "    #no_labels will be artifically reduced in the pipeline  later\n",
        "    w_stretch = w_train + augment_list(w_train, target_rate, 10 , 1) #\n",
        "    c_w_stretch = c_w_train + augment_list(c_w_train , target_rate, 10 , 1) \n",
        "    \n",
        "    #Split up cycles into sound clips with fixed lengths so they can be fed into a CNN\n",
        "    vtlp_alpha = [0.9,1.1]\n",
        "    vtlp_upper_freq = [3200,3800]\n",
        "    \n",
        "    train_none  = (split_and_pad_and_apply_mel_spect(none_train, desired_length, target_rate) +\n",
        "                   split_and_pad_and_apply_mel_spect(none_train, desired_length, target_rate, vtlp_alpha))\n",
        "    \n",
        "    train_c = (split_and_pad_and_apply_mel_spect(c_train, desired_length, target_rate) + \n",
        "               split_and_pad_and_apply_mel_spect(c_train, desired_length, target_rate, vtlp_alpha, vtlp_upper_freq, n_repeats = 3) ) #original samples + VTLP\n",
        "    \n",
        "    train_w = (split_and_pad_and_apply_mel_spect(w_stretch, desired_length, target_rate) + \n",
        "               split_and_pad_and_apply_mel_spect(w_stretch , desired_length, target_rate, vtlp_alpha , vtlp_upper_freq, n_repeats = 4)) #(original samples + time stretch) + VTLP\n",
        "    \n",
        "    train_c_w = (split_and_pad_and_apply_mel_spect(c_w_stretch, desired_length, target_rate) + \n",
        "                 split_and_pad_and_apply_mel_spect(c_w_stretch, desired_length, target_rate, vtlp_alpha , vtlp_upper_freq, n_repeats = 7)) #(original samples + time stretch * 2) + VTLP\n",
        "    \n",
        "    train_dict = {'none':train_none,'crackles':train_c,'wheezes':train_w, 'both':train_c_w}\n",
        "    \n",
        "    #test section \n",
        "    test_none  = split_and_pad_and_apply_mel_spect(none_test, desired_length, target_rate)\n",
        "    test_c = split_and_pad_and_apply_mel_spect(c_test, desired_length, target_rate)\n",
        "    test_w = split_and_pad_and_apply_mel_spect(w_test, desired_length, target_rate)\n",
        "    test_c_w = split_and_pad_and_apply_mel_spect(c_w_test, desired_length, target_rate)\n",
        "    \n",
        "    test_dict = {'none':test_none,'crackles':test_c,'wheezes':test_w, 'both':test_c_w}\n",
        "    \n",
        "    return [train_dict, test_dict]"
      ],
      "metadata": {
        "id": "zj9AahQVmIT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_sample_rate = 22000 \n",
        "sample_length_seconds = 5\n",
        "sample_dict = extract_all_training_samples(filenames, rec_annotations_dict, root, target_sample_rate, sample_length_seconds) #sample rate lowered to meet memory constraints\n",
        "training_clips = sample_dict[0]\n",
        "test_clips = sample_dict[1]"
      ],
      "metadata": {
        "id": "5bEO2AuxmV25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_sample_count(src_dict):\n",
        "    print('none:{}\\ncrackles:{}\\nwheezes:{}\\nboth:{}'.format(len(src_dict['none']),\n",
        "                                                        len(src_dict['crackles']),\n",
        "                                                        len(src_dict['wheezes']),\n",
        "                                                        len(src_dict['both'])))\n",
        "\n",
        "print('Samples Available')\n",
        "print('[Training set]')\n",
        "print_sample_count(training_clips)\n",
        "print('')\n",
        "print('[Test set]')\n",
        "print_sample_count(test_clips)"
      ],
      "metadata": {
        "id": "d3ST7ni2mbRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example of tiled sound samples\n",
        "sample_height = training_clips['none'][0][0].shape[0]\n",
        "sample_width = training_clips['none'][0][0].shape[1]\n",
        "ind = 1\n",
        "plt.figure(figsize = (10,10))\n",
        "plt.subplot(4,1,1)\n",
        "plt.imshow(training_clips['none'][ind][0].reshape(sample_height, sample_width))\n",
        "plt.title('None')\n",
        "plt.subplot(4,1,2)\n",
        "plt.imshow(training_clips['crackles'][ind][0].reshape(sample_height, sample_width))\n",
        "plt.title('Crackles')\n",
        "plt.subplot(4,1,3)\n",
        "plt.imshow(training_clips['wheezes'][ind][0].reshape(sample_height, sample_width))\n",
        "plt.title('Wheezes')\n",
        "plt.subplot(4,1,4)\n",
        "plt.imshow(training_clips['both'][ind][0].reshape(sample_height, sample_width))\n",
        "plt.title('Both')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "NGRp60GomfEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Pipeline"
      ],
      "metadata": {
        "id": "LYr62IfvmmOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.signal\n",
        "\n",
        "#Interleaved sampling between classes\n",
        "#Used to ensure a balance of classes for the training set\n",
        "class data_generator():\n",
        "    #sound_clips = [[none],[crackles],[wheezes],[both]]\n",
        "    #strides: How far the sampling index for each category is advanced for each step\n",
        "    def __init__(self, sound_clips, strides):\n",
        "        self.clips = sound_clips\n",
        "        self.strides = strides\n",
        "        self.lengths = [len(arr) for arr in sound_clips]\n",
        "    \n",
        "    def n_available_samples(self):\n",
        "        return int(min(np.divide(self.lengths, self.strides))) * 4\n",
        "    \n",
        "    def generate_keras(self, batch_size):\n",
        "        cursor = [0,0,0,0]\n",
        "        while True:\n",
        "            i = 0\n",
        "            X,y = [],[]\n",
        "            for c in range(batch_size):\n",
        "                cat_length = self.lengths[i]\n",
        "                cat_clips = self.clips[i]\n",
        "                cat_stride = self.strides[i]\n",
        "                cat_advance = np.random.randint(low= 1,high = cat_stride + 1)\n",
        "                clip = cat_clips[(cursor[i] + cat_advance) % cat_length]\n",
        "                cursor[i] = (cursor[i] + self.strides[i]) % cat_length #advance cursor\n",
        "                s = (self.rollFFT(clip))\n",
        "                X.append(s[0])\n",
        "                y.append(s[1])\n",
        "                i = (i + 1) % 4 # go to next class\n",
        "            yield (np.reshape(X, (batch_size, sample_height, sample_width, 1)),\n",
        "                   np.reshape(y,(batch_size,4)))\n",
        "\n",
        "    #Transpose and wrap each array along the time axis\n",
        "    def rollFFT(self, fft_info):\n",
        "        fft = fft_info[0]\n",
        "        n_col = fft.shape[1]\n",
        "        pivot = np.random.randint(n_col)\n",
        "        return ((np.roll(fft, pivot, axis = 1)), fft_info[1])\n",
        "\n",
        "#Used for validation set\n",
        "class feed_all():\n",
        "    #sound_clips = [[none],[crackles],[wheezes],[both]]\n",
        "    #strides: How far the sampling index for each category is advanced for each step\n",
        "    def __init__(self, sound_clips, roll = True):\n",
        "        merged = []\n",
        "        for arr in sound_clips:\n",
        "            merged.extend(arr)\n",
        "        np.random.shuffle(merged)\n",
        "        self.clips = merged\n",
        "        self.nclips = len(merged)\n",
        "        self.roll = roll\n",
        "    \n",
        "    def n_available_samples(self):\n",
        "        return len(self.clips)\n",
        "    \n",
        "    def generate_keras(self, batch_size):\n",
        "        i = 0\n",
        "        while True:\n",
        "            X,y = [],[]\n",
        "            for b in range(batch_size):\n",
        "                clip = self.clips[i]\n",
        "                i = (i + 1) % self.nclips\n",
        "                if(self.roll):\n",
        "                    s = (self.rollFFT(clip))\n",
        "                    X.append(s[0])\n",
        "                    y.append(s[1])\n",
        "                else:\n",
        "                    X.append(clip[0])\n",
        "                    y.append(clip[1])\n",
        "                    \n",
        "            yield (np.reshape(X, (batch_size,sample_height, sample_width,1)),\n",
        "                   np.reshape(y,(batch_size, 4)))\n",
        "\n",
        "    #Transpose and wrap each array along the time axis\n",
        "    def rollFFT(self, fft_info):\n",
        "        fft = fft_info[0]\n",
        "        n_col = fft.shape[1]\n",
        "        pivot = np.random.randint(n_col)\n",
        "        return ((np.roll(fft, pivot, axis = 1)), fft_info[1])"
      ],
      "metadata": {
        "id": "riOCine1mogk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[none_train, c_train, w_train, c_w_train] = [training_clips['none'], training_clips['crackles'], training_clips['wheezes'], training_clips['both']]\n",
        "[none_test, c_test, w_test,c_w_test] =  [test_clips['none'], test_clips['crackles'], test_clips['wheezes'], test_clips['both']]\n",
        "\n",
        "np.random.shuffle(none_train)\n",
        "np.random.shuffle(c_train)\n",
        "np.random.shuffle(w_train)\n",
        "np.random.shuffle(c_w_train)\n",
        "\n",
        "#Data pipeline objects\n",
        "train_gen = data_generator([none_train, c_train, w_train, c_w_train], [1,1,1,1])\n",
        "test_gen = feed_all([none_test, c_test, w_test,c_w_test])"
      ],
      "metadata": {
        "id": "y1YX3iQBmzaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN implementation"
      ],
      "metadata": {
        "id": "6U9uegYxm31p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1"
      ],
      "metadata": {
        "id": "Zfy8t0ClDyXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "n_epochs = 30"
      ],
      "metadata": {
        "id": "e5jezNJ9m6_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras implementation\n",
        "from keras import Sequential\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras.layers import Conv2D, Dense, Activation, Dropout, MaxPool2D, Flatten, LeakyReLU\n",
        "import tensorflow as tf\n",
        "K.clear_session()\n",
        "\n",
        "model_1 = Sequential()\n",
        "model_1.add(Conv2D(128, [7,11], strides = [2,2], padding = 'SAME', input_shape = (sample_height, sample_width, 1)))\n",
        "model_1.add(LeakyReLU(alpha = 0.1))\n",
        "model_1.add(MaxPool2D(padding = 'SAME')) # \"SAME\" = with zero padding\n",
        "\n",
        "model_1.add(Conv2D(256, [5,5], padding = 'SAME'))\n",
        "model_1.add(LeakyReLU(alpha = 0.1))\n",
        "model_1.add(MaxPool2D(padding = 'SAME'))\n",
        "\n",
        "model_1.add(Conv2D(256, [1,1], padding = 'SAME'))\n",
        "model_1.add(Conv2D(256, [3,3], padding = 'SAME'))\n",
        "model_1.add(LeakyReLU(alpha = 0.1))\n",
        "model_1.add(MaxPool2D(padding = 'SAME'))\n",
        "\n",
        "# model_1.add(Conv2D(512, [1,1], padding = 'SAME'))\n",
        "# model_1.add(Conv2D(512, [3,3], padding = 'SAME',activation = 'relu'))\n",
        "# model_1.add(MaxPool2D(padding = 'SAME'))\n",
        "\n",
        "model_1.add(Conv2D(512, [1,1], padding = 'SAME'))\n",
        "model_1.add(Conv2D(512, [3,3], padding = 'SAME', activation = 'relu'))\n",
        "model_1.add(MaxPool2D(padding = 'SAME'))\n",
        "model_1.add(Flatten())\n",
        "\n",
        "model_1.add(Dense(4096, activation = 'relu'))\n",
        "model_1.add(Dropout(0.5))\n",
        "\n",
        "model_1.add(Dense(512, activation = 'relu'))\n",
        "model_1.add(Dense(4, activation = 'softmax'))\n",
        "\n",
        "# \"SAME\" tries to pad evenly left and right, but if the amount of columns to be added is odd, \n",
        "# it will add the extra column to the right, as is the case in this example \n",
        "# (the same logic applies vertically: there may be an extra row of zeros at the bottom).\n",
        "\n",
        "opt = optimizers.Adam(learning_rate=0.0001, \n",
        "                      beta_1=0.9, \n",
        "                      beta_2=0.999, \n",
        "                      epsilon=None, \n",
        "                      decay=0.00, \n",
        "                      amsgrad=False)\n",
        "\n",
        "model_1.compile(optimizer=opt, \n",
        "                loss='categorical_crossentropy', \n",
        "                metrics=['acc'])\n",
        "\n",
        "log_dir = \"logs/fit/model_1\"\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n",
        "                                                      histogram_freq=1)"
      ],
      "metadata": {
        "id": "iug2cIzInAko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "plot_model(model_1, show_shapes=True, show_layer_names = True)"
      ],
      "metadata": {
        "id": "4Qm6YxvgnEba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats = model_1.fit_generator(generator = train_gen.generate_keras(batch_size), \n",
        "                            steps_per_epoch = train_gen.n_available_samples() // batch_size,\n",
        "                            validation_data = test_gen.generate_keras(batch_size),\n",
        "                            validation_steps = test_gen.n_available_samples() // batch_size, \n",
        "                            epochs = n_epochs,\n",
        "                            callbacks = tensorboard_callback)"
      ],
      "metadata": {
        "id": "fK4YVREWnVwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(stats.history['acc'], label = 'training acc')\n",
        "plt.plot(stats.history['val_acc'], label = 'validation acc')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(stats.history['loss'], label = 'training loss')\n",
        "plt.plot(stats.history['val_loss'], label = 'validation loss')\n",
        "plt.legend()\n",
        "plt.suptitle('Model 1', fontsize=16)\n",
        "plt.title('Loss')"
      ],
      "metadata": {
        "id": "PpFxIalDnZgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = test_gen.generate_keras(test_gen.n_available_samples()).__next__()\n",
        "predictions = model_1.predict(test_set[0])\n",
        "predictions = np.argmax(predictions, axis = 1)\n",
        "labels = np.argmax(test_set[1], axis = 1)"
      ],
      "metadata": {
        "id": "R86wWMwsncOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(labels, predictions, target_names = ['none','crackles','wheezes','both']))\n",
        "print(confusion_matrix(labels, predictions))"
      ],
      "metadata": {
        "id": "dyxKy-zKndkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2"
      ],
      "metadata": {
        "id": "Ug2tA5DKECWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "n_epochs = 30"
      ],
      "metadata": {
        "id": "knnEiPh9D_Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras implementation\n",
        "from keras import Sequential\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras.layers import Conv2D, Dense, Activation, Dropout, MaxPool2D, Flatten, LeakyReLU\n",
        "import tensorflow as tf\n",
        "K.clear_session()\n",
        "\n",
        "model_2 = Sequential()\n",
        "model_2.add(Conv2D(128, [7,11], strides = [2,2], padding = 'SAME', input_shape = (sample_height, sample_width, 1)))\n",
        "model_2.add(LeakyReLU(alpha = 0.1))\n",
        "model_2.add(MaxPool2D(padding = 'SAME')) # \"SAME\" = with zero padding\n",
        "\n",
        "model_2.add(Conv2D(256, [5,5], padding = 'SAME'))\n",
        "model_2.add(LeakyReLU(alpha = 0.1))\n",
        "model_2.add(MaxPool2D(padding = 'SAME'))\n",
        "\n",
        "model_2.add(Conv2D(256, [1,1], padding = 'SAME'))\n",
        "model_2.add(Conv2D(256, [3,3], padding = 'SAME'))\n",
        "model_2.add(LeakyReLU(alpha = 0.1))\n",
        "model_2.add(MaxPool2D(padding = 'SAME'))\n",
        "\n",
        "model_2.add(Conv2D(512, [1,1], padding = 'SAME'))\n",
        "model_2.add(Conv2D(512, [3,3], padding = 'SAME',activation = 'relu'))\n",
        "# model_2.add(MaxPool2D(padding = 'SAME'))\n",
        "\n",
        "model_2.add(Conv2D(512, [1,1], padding = 'SAME'))\n",
        "model_2.add(Conv2D(512, [3,3], padding = 'SAME', activation = 'relu'))\n",
        "model_2.add(MaxPool2D(padding = 'SAME'))\n",
        "model_2.add(Flatten())\n",
        "\n",
        "model_2.add(Dense(4096, activation = 'relu'))\n",
        "model_2.add(Dropout(0.5))\n",
        "\n",
        "model_2.add(Dense(512, activation = 'relu'))\n",
        "model_2.add(Dense(4, activation = 'softmax'))\n",
        "\n",
        "# \"SAME\" tries to pad evenly left and right, but if the amount of columns to be added is odd, \n",
        "# it will add the extra column to the right, as is the case in this example \n",
        "# (the same logic applies vertically: there may be an extra row of zeros at the bottom).\n",
        "\n",
        "opt = optimizers.Adam(learning_rate=0.0001, \n",
        "                      beta_1=0.9, \n",
        "                      beta_2=0.999, \n",
        "                      epsilon=None, \n",
        "                      decay=0.00, \n",
        "                      amsgrad=False)\n",
        "\n",
        "model_2.compile(optimizer=opt, \n",
        "                loss='categorical_crossentropy', \n",
        "                metrics=['acc'])\n",
        "\n",
        "log_dir = \"logs/fit/model_2\"\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n",
        "                                                      histogram_freq=1)"
      ],
      "metadata": {
        "id": "TtSaDP4KE7-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "plot_model(model_2, show_shapes=True, show_layer_names = True)"
      ],
      "metadata": {
        "id": "DZzGzsVqFqvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats = model_2.fit_generator(generator = train_gen.generate_keras(batch_size), \n",
        "                            steps_per_epoch = train_gen.n_available_samples() // batch_size,\n",
        "                            validation_data = test_gen.generate_keras(batch_size),\n",
        "                            validation_steps = test_gen.n_available_samples() // batch_size, \n",
        "                            epochs = n_epochs,\n",
        "                            callbacks = tensorboard_callback)"
      ],
      "metadata": {
        "id": "yze5q7m9F8Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(stats.history['acc'], label = 'training acc')\n",
        "plt.plot(stats.history['val_acc'], label = 'validation acc')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(stats.history['loss'], label = 'training loss')\n",
        "plt.plot(stats.history['val_loss'], label = 'validation loss')\n",
        "plt.legend()\n",
        "plt.suptitle('Model 2', fontsize=16)\n",
        "plt.title('Loss')"
      ],
      "metadata": {
        "id": "f74iaUbk8N8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = test_gen.generate_keras(test_gen.n_available_samples()).__next__()\n",
        "predictions = model_2.predict(test_set[0])\n",
        "predictions = np.argmax(predictions, axis = 1)\n",
        "labels = np.argmax(test_set[1], axis = 1)"
      ],
      "metadata": {
        "id": "Fh4wXskHF_od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(labels, predictions, target_names = ['none','crackles','wheezes','both']))\n",
        "print(confusion_matrix(labels, predictions))"
      ],
      "metadata": {
        "id": "lX9CJ8o1GIEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3"
      ],
      "metadata": {
        "id": "VY79NukHEF3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "n_epochs = 30"
      ],
      "metadata": {
        "id": "TW6vrTi-Gka_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras implementation\n",
        "from keras import Sequential\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras.layers import Conv2D, Dense, Activation, Dropout, MaxPool2D, Flatten, LeakyReLU\n",
        "import tensorflow as tf\n",
        "K.clear_session()\n",
        "\n",
        "model_3 = Sequential()\n",
        "model_3.add(Conv2D(128, [7,11], strides = [2,2], padding = 'SAME', input_shape = (sample_height, sample_width, 1)))\n",
        "model_3.add(LeakyReLU(alpha = 0.1))\n",
        "model_3.add(MaxPool2D(padding = 'SAME')) # \"SAME\" = with zero padding\n",
        "\n",
        "model_3.add(Conv2D(256, [5,5], padding = 'SAME'))\n",
        "model_3.add(LeakyReLU(alpha = 0.1))\n",
        "model_3.add(MaxPool2D(padding = 'SAME'))\n",
        "\n",
        "model_3.add(Conv2D(256, [1,1], padding = 'SAME'))\n",
        "model_3.add(Conv2D(256, [3,3], padding = 'SAME'))\n",
        "model_3.add(LeakyReLU(alpha = 0.1))\n",
        "model_3.add(MaxPool2D(padding = 'SAME'))\n",
        "\n",
        "model_3.add(Conv2D(512, [1,1], padding = 'SAME'))\n",
        "model_3.add(Conv2D(512, [3,3], padding = 'SAME',activation = 'relu'))\n",
        "# model_3.add(MaxPool2D(padding = 'SAME'))\n",
        "\n",
        "model_3.add(Conv2D(512, [1,1], padding = 'SAME'))\n",
        "model_3.add(Conv2D(512, [3,3], padding = 'SAME', activation = 'relu'))\n",
        "model_3.add(MaxPool2D(padding = 'SAME'))\n",
        "model_3.add(Flatten())\n",
        "\n",
        "model_3.add(Dense(4096, activation = 'relu'))\n",
        "model_3.add(Dropout(0.5))\n",
        "\n",
        "model_3.add(Dense(512, activation = 'relu'))\n",
        "model_3.add(Dense(4, activation = 'softmax'))\n",
        "\n",
        "# \"SAME\" tries to pad evenly left and right, but if the amount of columns to be added is odd, \n",
        "# it will add the extra column to the right, as is the case in this example \n",
        "# (the same logic applies vertically: there may be an extra row of zeros at the bottom).\n",
        "\n",
        "opt = optimizers.Adam(learning_rate=0.0001, \n",
        "                      beta_1=0.9, \n",
        "                      beta_2=0.999, \n",
        "                      epsilon=None, \n",
        "                      decay=0.00, \n",
        "                      amsgrad=False)\n",
        "\n",
        "model_3.compile(optimizer=opt, \n",
        "                loss='categorical_crossentropy', \n",
        "                metrics=['acc'])\n",
        "\n",
        "log_dir = \"logs/fit/model_3\"\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n",
        "                                                      histogram_freq=1)"
      ],
      "metadata": {
        "id": "ZJyN3uqvGnex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "plot_model(model_2, show_shapes=True, show_layer_names = True)"
      ],
      "metadata": {
        "id": "iDKI0_blGr7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats = model_2.fit_generator(generator = train_gen.generate_keras(batch_size), \n",
        "                            steps_per_epoch = train_gen.n_available_samples() // batch_size,\n",
        "                            validation_data = test_gen.generate_keras(batch_size),\n",
        "                            validation_steps = test_gen.n_available_samples() // batch_size, \n",
        "                            epochs = n_epochs,\n",
        "                            callbacks = tensorboard_callback)"
      ],
      "metadata": {
        "id": "9dK_Mdz5Gu7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(stats.history['acc'], label = 'training acc')\n",
        "plt.plot(stats.history['val_acc'], label = 'validation acc')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(stats.history['loss'], label = 'training loss')\n",
        "plt.plot(stats.history['val_loss'], label = 'validation loss')\n",
        "plt.legend()\n",
        "plt.suptitle('Model 3', fontsize=16)\n",
        "plt.title('Loss')"
      ],
      "metadata": {
        "id": "PnXYgFvH0NGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = test_gen.generate_keras(test_gen.n_available_samples()).__next__()\n",
        "predictions = model_2.predict(test_set[0])\n",
        "predictions = np.argmax(predictions, axis = 1)\n",
        "labels = np.argmax(test_set[1], axis = 1)"
      ],
      "metadata": {
        "id": "CgHtKc25Gxlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(labels, predictions, target_names = ['none','crackles','wheezes','both']))\n",
        "print(confusion_matrix(labels, predictions))"
      ],
      "metadata": {
        "id": "dPAvX-afG0ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorboard"
      ],
      "metadata": {
        "id": "QVmDGpKC1teG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load tensorboard\n",
        "%load_ext tensorboard\n",
        "# start tensorboard inside the notebook. Outside of the notebook, \n",
        "# use tensorboard --logdir=./logs\n",
        "%tensorboard --logdir=./logs"
      ],
      "metadata": {
        "id": "4LMt8c6N1nOx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}